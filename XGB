import pandas as pd
import numpy as np
from xgboost import XGBRegressor
from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import GridSearchCV
from matplotlib import pyplot
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import mean_absolute_error
df_=pd.read_csv("C:\\Users\\User\\Desktop\\dataset2.csv")
df=df_.copy()
df.head()
X = df.drop('tensile_strength', axis=1)
y = df['tensile_strength']
df.head()
X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=42)
X_train.shape
xgb_model=XGBRegressor().fit(X_train,y_train)
y_pred=xgb_model.predict(X_train)
y_pred = xgb_model.predict(X_train)
r2 = r2_score(y_train, y_pred)
print("r2train :" + str(r2))
y_pred=xgb_model.predict(X_test)
xgb_grid = {
     'colsample_bytree': [0.4, 0.5,0.6], 
     'n_estimators':[50, 100],
     'max_depth': [2,3,4],
     'learning_rate': [0.1, 0.01, 0.5]
}
xgb = XGBRegressor()
xgb_cv = GridSearchCV(xgb, 
                      param_grid = xgb_grid, 
                      cv = 10, 
                      n_jobs = -1,
                      verbose = 2)
xgb_cv.fit(X_train, y_train)
xgb_cv.best_params_
xgb_tuned = XGBRegressor(colsample_bytree = 0.4, 
                         learning_rate = 0.1, 
                         max_depth = 4, 
                         n_estimators = 100) 

xgb_tuned = xgb_tuned.fit(X_train,y_train)
y_pred = xgb_tuned.predict(X_test)
y_predict = xgb_tuned.predict(X)
R2 = r2_score(y_test, y_pred)
r2 = r2_score(y, y_predict)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
sns.regplot(y_test, y_pred, color='b', label=f"RMSE: {rmse:.2f}, r2: {r2:.2f}")
plt.xlabel('Test Values of T.S. MPa')
plt.ylabel('Predicted Values of T.S. MPa')
plt.legend()
plt.show()
sns.regplot(y, y_predict, color='b', label=f"RMSE: {rmse:.2f}, r2: {r2:.2f}")
plt.xlabel('Train Values of T.S. MPa')
plt.ylabel('Predicted Values of T.S. MPa')
plt.legend()
plt.show()
mae = mean_absolute_error(y_test, y_pred)
print("MAE değeri:", mae)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
print("rmse :" + str(rmse))
a = [220, 60, 0.8, 20]
a = np.array(a) # convert to a numpy array
a = np.expand_dims(a, 0) # change shape from (8,) to (1,8)
xgb_tuned.predict(a)
print("pred xgb değeri:", xgb_tuned.predict(a))
y_pred = xgb_model.predict(X_train)
r2 = r2_score(y_train, y_pred)
y_pred = xgb_model.predict(X_test)
y_predict = xgb_model.predict(X_train)
r2 = r2_score(y_test, y_pred)
print("r2trainxgb :" + str(r2))
plt.title('XGBoost')
plt.xlabel('True Values')
plt.ylabel('Prediction')
plt.legend()
plt.show()
df = pd.DataFrame({"True values":y_test, "Prediction":y_pred}, index=X_test.index)
df.sort_index(inplace=True)
plt.scatter(df.index, df["True values"], marker="o", label="Test values")
plt.plot(df.index, df["Prediction"], color="r", label="Prediction")
plt.xlabel("Index")
plt.ylabel("Values")
plt.legend()
plt.show()
Importance=pd.DataFrame({"Importance":xgb_tuned.feature_importances_*100},
                       index=X_train.columns)
Importance.sort_values(by="Importance",
                      axis=0,
                      ascending=True).plot(kind="barh",color="g")
sorted_data = Importance.sort_values(by="Importance", axis=0, ascending=True)
ax = sorted_data.plot(kind="barh", color="b")
ax.set_xlabel("Importance/ %")
ax.set_ylabel("Features")
print("y_pred list", y_pred)
